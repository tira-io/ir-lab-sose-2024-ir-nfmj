{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and the Index\n",
    "\n",
    "The type of the index object that we load is `<class 'jnius.reflect.org.terrier.structures.Index'>`, in fact a [Java class](http://terrier.org/docs/v3.6/javadoc/org/terrier/structures/Index.html) wrapped into Python. However, you do not need to worry about this: at this point, we will simply use the provided Index object to run procedures defined in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "document_entity_recognition = tira.pt.transform_documents('ir-lab-sose-2024/ir-nfmj/entity-recognition', pt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 60669.18it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_with_entities = []\n",
    "\n",
    "for i in pt_dataset.get_corpus_iter():\n",
    "    corpus_with_entities += [{'docno': i['docno']}]\n",
    "\n",
    "corpus_with_entities = document_entity_recognition(pd.DataFrame(corpus_with_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_only_entity_type_text(positive_entity_types):\n",
    "    ret = []\n",
    "    for _, i in corpus_with_entities.iterrows():\n",
    "        text = \"\"\n",
    "        for j in i[\"entities\"]:\n",
    "            if j[\"label\"] in positive_entity_types:\n",
    "                text += \" \" + j[\"text\"]\n",
    "        if len(text) > 0:\n",
    "            ret += [{\"docno\": i[\"docno\"], \"text\": text}]     \n",
    "    return ret        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "alle Entitäten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:  70%|███████   | 88895/126958 [00:10<00:04, 7763.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:26:41.986 [ForkJoinPool-2-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (2020.mir_conference-2020.1) - further warnings are suppressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:14<00:00, 8881.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:26:47.521 [ForkJoinPool-2-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 3 empty documents\n"
     ]
    }
   ],
   "source": [
    "indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, stemmer=None, meta={'docno': 75, 'text': 4096})\n",
    "\n",
    "index_full_text = pt.IndexFactory.of(indexer.index(pt_dataset.get_corpus_iter()))\n",
    "bm25 = pt.BatchRetrieve(index_full_text, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:26:52.024 [ForkJoinPool-3-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (D12-1116) - further warnings are suppressed\n",
      "08:26:52.192 [ForkJoinPool-3-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 3 empty documents\n"
     ]
    }
   ],
   "source": [
    "indexer = pt.IterDictIndexer(\"/tmp/index-EVENT\", overwrite=True, stemmer=None, meta={'docno': 75, 'text': 4096})\n",
    "\n",
    "index_event_text = pt.IndexFactory.of(indexer.index(retain_only_entity_type_text(set(['EVENT']))))\n",
    "bm25_event = pt.BatchRetrieve(index_event_text, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Resultat:\n",
    "-----\n",
    "Effektiv: ORG, PERSON, WORK_OF_ART, NORP, PRODUCT, FAC, LOC, MONEY, EVENT, LAW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:26:56.938 [ForkJoinPool-4-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (W11-4616) - further warnings are suppressed\n",
      "08:27:00.338 [ForkJoinPool-4-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 202 empty documents\n"
     ]
    }
   ],
   "source": [
    "indexer = pt.IterDictIndexer(\"/tmp/index-MULTIPLE_TYPES\", overwrite=True, stemmer=None, meta={'docno': 75, 'text': 4096})\n",
    "\n",
    "index_with_entities = pt.IndexFactory.of(indexer.index(retain_only_entity_type_text(set(['ORG', 'PERSON', 'WORK_OF_ART', 'NORP', 'PRODUCT', 'FAC', 'LOC', 'MONEY', 'EVENT', 'LAW']))))\n",
    "bm25_ents = pt.BatchRetrieve(index_with_entities, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Try Improvements\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pipe = (bm25*0.99) + (bm25_ents*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n"
     ]
    }
   ],
   "source": [
    "run = entity_pipe(pt_dataset.get_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-entities', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
