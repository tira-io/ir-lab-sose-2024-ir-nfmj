{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tira in /usr/local/lib/python3.10/dist-packages (0.0.132)\n",
      "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /usr/local/lib/python3.10/dist-packages (from tira) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tira) (4.66.1)\n",
      "Requirement already satisfied: docker==6.*,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tira) (6.1.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tira) (2.1.3)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (1.7.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (2.1.0)\n",
      "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (23.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (1.26.2)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.3.2)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.3)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.3.2)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (6.0.1)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.12)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.6)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.12.2)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (3.2.3)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.9.3)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/dist-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->python-terrier) (3.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->python-terrier) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# You only need to execute this cell if you are using Google Golab.\n",
    "# If you use GitHub Codespaces, everything is already installed.\n",
    "!pip3 install tira ir-datasets python-terrier\n",
    "!pip3 install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stopwords():\n",
    "    file_path = '../stopwords/stopwordlist.txt' # Default Stopword List by Terrier\n",
    "\n",
    "    stopwords_list = []\n",
    "\n",
    "    # Öffne die Datei und lese jede Zeile\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            stripped_line = line.strip()\n",
    "            if \"information\" in stripped_line: # Ignoriere Stopwort Information\n",
    "                continue \n",
    "            if stripped_line:\n",
    "                stopwords_list.append(stripped_line)\n",
    "    return stopwords_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n",
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:  71%|███████   | 90188/126958 [00:23<00:06, 5739.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:29:26.206 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (2020.mir_conference-2020.1) - further warnings are suppressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:29<00:00, 4242.80it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:29:37.467 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 1 empty documents\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "iter_indexer = pt.IterDictIndexer(\"../index\", stopwords=custom_stopwords(),meta={'docno': 50, 'text': 4096}, overwrite=True, blocks=True)\n",
    "\n",
    "index = iter_indexer.index(pt_dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Retrieval Pipeline and load the Dataset and the Index\n",
    "\n",
    "The type of the index object that we load is `<class 'jnius.reflect.org.terrier.structures.Index'>`, in fact a [Java class](http://terrier.org/docs/v3.6/javadoc/org/terrier/structures/Index.html) wrapped into Python. However, you do not need to worry about this: at this point, we will simply use the provided Index object to run procedures defined in Python.\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing Query from [machine learning language identification] to [\"language identification\" \"machine learning\"]\n",
      "Changing Query from [social media detect self-harm] to [\"social media\" detect self-harm]\n",
      "Changing Query from [Pseudo-relevance feedback] to [\"relevance feedback\" pseudo-relevance]\n",
      "Changing Query from [Algorithm acceleration with Nvidia CUDA] to [\"nvidia cuda\" algorithm acceleration with]\n",
      "Changing Query from [Inclusion of text-mining] to [\"text mining\" inclusion of text-mining]\n",
      "Changing Query from [The Ethics of Artificial Intelligence] to [\"ethics of artificial intelligence\" the]\n",
      "Changing Query from [machine learning for more relevant results] to [\"machine learning\" for more relevant results]\n",
      "Changing Query from [Crawling websites using machine learning] to [\"machine learning\" crawling websites using]\n",
      "Changing Query from [Consumer Product reviews] to [\"consumer product\" reviews]\n",
      "Changing Query from [Limitations machine learning] to [\"machine learning\" limitations]\n",
      "Changing Query from [Natural Language Processing] to [\"natural language processing\"]\n",
      "Changing Query from [medical studies that use information retrieval ] to [\"information retrieval\" \"medical studies\" that use]\n",
      "Changing Query from [information retrieval on different language sources ] to [\"information retrieval\" different language sources]\n",
      "Changing Query from [papers that compare multiple information retrieval methods] to [\"information retrieval\" papers that compare multiple methods]\n",
      "Changing Query from [ risks of information retrieval in social media ] to [\"information retrieval\" \"social media\" risks of]\n",
      "Changing Query from [fake news detection] to [\"fake news\" detection]\n",
      "Changing Query from [processing natural language for information retrieval] to [\"information retrieval\" \"natural language\" processing]\n",
      "Changing Query from [sentiment analysis] to [\"sentiment analysis\"]\n",
      "Changing Query from [Query log analysis] to [\"log analysis\" query]\n",
      "Changing Query from [Deep Neural Networks] to [\"deep neural networks\"]\n",
      "Changing Query from [Information Retrieval] to [\"information retrieval\"]\n",
      "Changing Query from [The University of Amsterdam] to [\"university of amsterdam\" the]\n",
      "Changing Query from [web pages evolution] to [\"web pages\" evolution]\n",
      "Changing Query from [query optimization] to [\"query optimization\"]\n",
      "Changing Query from [ Cosine similarity vector ] to [\"cosine similarity\" vector]\n",
      "Changing Query from [search engine optimization with query logs] to [\"search engine optimization\" with query logs]\n",
      "Changing Query from [ What makes Natural Language Processing natural ] to [\"natural language processing\" what makes]\n",
      "Changing Query from [principle of a information retrieval indexing] to [\"information retrieval\" principle of indexing]\n",
      "Changing Query from [Architecture of web search engine] to [\"web search engine\" architecture of]\n",
      "Changing Query from [What is information retrieval] to [\"information retrieval\" what is]\n",
      "Changing Query from [Information Retrieval with algorithms] to [\"information retrieval\" with algorithms]\n",
      "Changing Query from [Advancements in Information Retrieval] to [\"information retrieval\" advancements]\n"
     ]
    }
   ],
   "source": [
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "\n",
    "topics = pt_dataset.get_topics(variant='title')\n",
    "\n",
    "query_entity_linking = tira.pt.transform_queries('ir-benchmarks/marcel-gohsen/entity-linking', pt_dataset)\n",
    "\n",
    "linked_queries = query_entity_linking(topics)\n",
    "\n",
    "bm25 = pt.BatchRetrieve(\"../index\", wmodel=\"BM25\")\n",
    "\n",
    "def remove_questionmark_from_queries():\n",
    "    for index, row in linked_queries.iterrows():\n",
    "        if '?' in str(row['query']):\n",
    "            linked_queries['query'] = linked_queries['query'].str.replace('?', '')\n",
    "\n",
    "def keyphrase_containment_checker(input_phrase, wanted_string):\n",
    "    # Überprüfen, ob der gewollte String bereits in der Input Phrase enthalten ist\n",
    "    if wanted_string in input_phrase:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def entity_keyphrase_length_checker(entity_keyphrase):\n",
    "    # String nach Leerzeichen aufsplitten\n",
    "    entity_word = entity_keyphrase.split(\" \")\n",
    "    # Anzahl der Elemente ermitteln\n",
    "    return len(entity_word)\n",
    "\n",
    "def add_missing_terms(input_phrase, wanted_string):\n",
    "    wanted_string = wanted_string.lower()\n",
    "    if not keyphrase_containment_checker(input_phrase, wanted_string):\n",
    "        input_phrase += wanted_string + f\" \"\n",
    "        return input_phrase\n",
    "    else:\n",
    "        return input_phrase\n",
    "\n",
    "\n",
    "def query_rewrite(linked_queries, score_threshold=0.9, entity_list_size=2):\n",
    "    queries_entities = linked_queries['entities'].to_dict()\n",
    "    for qid in queries_entities:\n",
    "        query = linked_queries['query'][qid];\n",
    "        query_entities = queries_entities[qid] # Eine Liste mit entitäten, die von jeder Query kommt\n",
    "        if(len(query_entities) > 0): # Manche Query besitzen vielleicht keine Entitäten\n",
    "            keyphrases = {} # Dict für die Keyphrases\n",
    "\n",
    "            j = 0;\n",
    "            for i in range(0,entity_list_size):\n",
    "                if(entity_list_size < len(query_entities)):\n",
    "                    entity_keyphrase = query_entities[i]['mention']\n",
    "                    entity_score = query_entities[i]['score']\n",
    "                    keyphrases[i] = (entity_keyphrase, entity_score) # keyphrases[i][0] := Entität und keyphrases[i][1] := Der Score der Entität\n",
    "                else:                                                # Wenn entity_list_size zu groß ist für len(query_entities), dann nehme die maximale mögliche anzahl, also len(query_entities)\n",
    "                    entity_keyphrase = query_entities[i]['mention']\n",
    "                    entity_score = query_entities[i]['score']\n",
    "                    keyphrases[i] = (entity_keyphrase, entity_score)\n",
    "                    j += 1\n",
    "                    if(j == len(query_entities)):\n",
    "                        break\n",
    "            \n",
    "\n",
    "            delta = 0 # Mittelwert des Scores von den einzelen Entitäten aus einer Query\n",
    "            for i in range(0,len(keyphrases)):\n",
    "                delta += keyphrases[i][1]\n",
    "            delta = delta / len(keyphrases)\n",
    "            \n",
    "            result = \"\"\n",
    "            old_result = \"\"\n",
    "            if( delta >= score_threshold):\n",
    "                for i in range(0,len(keyphrases)):\n",
    "                    phrase = keyphrases[i][0]\n",
    "                    if entity_keyphrase_length_checker(phrase) >= 2 and not keyphrase_containment_checker(old_result, phrase):\n",
    "                        result += f'\"{phrase}\"' + f\" \"\n",
    "                        old_result = result\n",
    "                \n",
    "                if len(result) != 0:\n",
    "                    query_word_list = query.split(\" \")\n",
    "                    for i in range(0, len(query_word_list)):\n",
    "                        result = add_missing_terms(result, query_word_list[i])\n",
    "                    result = result.strip(\" \");\n",
    "                    print(\"Changing Query from \" +  '[' + linked_queries['query'][qid] + ']' + \" to [\" + result + \"]\")\n",
    "                    linked_queries['query'][qid] = result\n",
    "remove_questionmark_from_queries() \n",
    "query_rewrite(linked_queries, 0.2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_461/3078423960.py:1: DeprecationWarning: Coercion of a dataframe into a transformer is deprecated; use a pt.Transformer.from_df() instead\n",
      "  bm25QR = linked_queries >> bm25\n"
     ]
    }
   ],
   "source": [
    "bm25QR = linked_queries >> bm25 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "runDefault = bm25(pt_dataset.get_topics('text'))\n",
    "runQR = bm25QR(pt_dataset.get_topics('text'))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs/defaultRuns\".\n",
      "Done. run file is stored under \"../runs/defaultRuns/run.txt\".\n",
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs/QRRuns\".\n",
      "Done. run file is stored under \"../runs/QRRuns/run.txt\".\n",
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(runDefault,  system_name='bm25-baseline', default_output='../runs/defaultRuns')\n",
    "persist_and_normalize_run(runQR, system_name='bm25-baseline', default_output='../runs/QRRuns')\n",
    "persist_and_normalize_run(runQR, system_name='bm25-baseline', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
